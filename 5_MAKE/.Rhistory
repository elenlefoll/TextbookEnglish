category = spoken_sem$Semantics[i]
row = spoken_sem[spoken_sem$Semantics==category,cols]
other_rows = spoken_sem[spoken_sem$Semantics!=category,cols]
collapsed = rbind(row, colSums(other_rows))
category_test = fisher.test(collapsed)
results$p[i] = category_test$p.value
results$OddsRatio[i] = category_test$estimate
results$conf.int.lower[i] = category_test$conf.int[1]
results$conf.int.upper[i] = category_test$conf.int[2]
}
results
results$p = p.adjust(results$p, method='none')
results$Star <- stars.pval(results$p)
results = results %>%
mutate(Star = replace(Star, Star==".", "˘")) # Change dot to a higher symbol such as º or ˘
#svg(filename=here("MAKE_produce_Spoken.svg"), width=14, height=7, pointsize=12)
fig = ggplot(spoken_sem) +
geom_segment(aes(x=Semantics, xend=Semantics, y=BNCspokenPer, yend=TxBspokenPer), color="black") +
geom_point(aes(x=Semantics, y=BNCspokenPer), color=rgb(153,0,0, max = 255, alpha = 255), size=5) +
geom_point(aes(x=Semantics, y=TxBspokenPer), color=rgb(252, 141, 89, max = 255, alpha = 220), size=5) +
coord_flip() +
theme_minimal() +
theme(
panel.border = element_blank(),
axis.text=element_text(size=16, colour = "black"),
axis.title.x=element_text(size=16, face = "bold")
) +
scale_x_discrete(limits=c(levels(spoken_sem$Semantics),"")) +
xlab("") +
ylab("% of MAKE collocations in the (Ai) produce sense") +  # Adds an empty factor to give space for legend as recommended in: https://stackoverflow.com/questions/16788180/r-ggplot-extend-the-range-of-a-category-x-axis
theme(plot.margin=margin(5,5,5,5))
# +  labs(title = "Differences in semantic fields of collocates of MAKE in the produce sense (Ai).")
fig1 = fig +
geom_text(data=data.frame(), aes(x="Sports, Entertainment & Travel", y=28, label="Textbook Conversation"),
color=rgb(252, 141, 89, max = 255, alpha = 255), hjust=1, size=6, fontface = "bold", nudge_x = 0.7) +
geom_text(data=data.frame(), aes(x="Sports, Entertainment & Travel", y= 1, label="Spoken BNC 2014 sample"),
color=rgb(153,0,0, max = 255, alpha = 255), hjust=0, size=6, fontface = "bold", nudge_x = 0.7)
## This is to help us place the stars in the middle of the bars ##
cols = c("BNCspokenPer", "TxBspokenPer")
results$y = rowMeans(spoken_sem[,cols])
fig2 = fig1 +
geom_text(aes(x=Semantics, y=y, label=Star), data=results, size=9)
print(fig2)
dev.off()
#ggsave(here("produce_USAS_Conversation.png"), width = 24, height = 12, units = "cm", dpi = 300)
#### Chatterplot: inspired by https://towardsdatascience.com/rip-wordclouds-long-live-chatterplots-e76a76896098
produce <- read.csv(file = here("Produce.csv"), sep = "\t", stringsAsFactors = TRUE)
summary(produce$USAS)
str(produce)
spoken_produce <- produce %>% filter(Subcorpus=="BNCspoken" | Subcorpus =="spoken")
spoken_produce$Subcorpus <- droplevels(spoken_produce$Subcorpus)
table(spoken_produce$Collocate_lemma, spoken_produce$Subcorpus)
comp <- round(prop.table(table(spoken_produce$Collocate_lemma, spoken_produce$Subcorpus), 2), 4)*100
comp <- as.data.frame(unclass(comp))
comp$Collocate_lemma <- row.names(comp)
head(comp)
comp <- inner_join(comp, spoken_produce[,6:7], by = "Collocate_lemma")
comp <- distinct(comp)
head(comp)
suffrager <- c(palettes_d$suffrager$classic[1], palettes_d$suffrager$CarolMan[3:4], palettes_d$suffrager$london, palettes_d$suffrager$oxon[c(1,5)])
levels(as.factor(comp$spoken))
## With minimal threshold of just min 2 occurrences
comp %>% filter(spoken > 0.57 | BNCspoken > 0.57) %>%
ggplot(aes(spoken, BNCspoken, label = Collocate_lemma)) +
# ggrepel geom, make arrows transparent, color by rank, size by n
geom_point(aes(colour=USAS)) +
geom_text_repel(min.segment.length = 0.3, segment.alpha = 0.2, force = 0.4,
aes(colour=USAS), show_guide = F) +
scale_colour_manual(values = suffrager,
name = "Semantic field") +
# set color gradient & customize legend
geom_abline(color = "gray40", lty = 2) +
# set word size range & turn off legend
labs(y = "% of MAKE 'produce' collocations in Spoken BNC2014 sample", x = "% of 'produce' MAKEs in Textbook Conversation") +
scale_y_log10(breaks=c(1,2,3,4)) +
scale_x_log10(breaks=c(1,2,4,6,8,10,12)) +
# minimal theme & customizations
theme_bw() +
theme(legend.position=c(0.86,0.35),
#legend.justification = c("right","top"),
panel.grid.major = element_line(colour = "whitesmoke"),
panel.grid.minor=element_blank(),
legend.background = element_rect(colour = 'darkgrey', fill = 'white', linetype='solid'))
#ggsave(here("Produce_MAKEs_Spoken_chatterplot.svg"), dpi = 300, width =22, height = 15, units = "cm")
## With higher threshold of min. 3 occurrences
comp %>% filter(spoken > 1.04 | BNCspoken > 1.11) %>%
ggplot(aes(spoken, BNCspoken, label = Collocate_lemma)) +
# ggrepel geom, make arrows transparent, color by rank, size by n
geom_point(aes(colour=USAS)) +
geom_text_repel(min.segment.length = 0.3, segment.alpha = 0.2, force = 0.4,
aes(colour=USAS), show_guide = F) +
scale_colour_manual(values = suffrager,
name = "Semantic field") +
# set color gradient & customize legend
geom_abline(color = "gray40", lty = 2) +
# set word size range & turn off legend
labs(y = "% of MAKE 'produce' collocations in Spoken BNC2014 sample", x = "% of 'produce' MAKEs in Textbook Conversation") +
scale_y_log10(breaks=c(1,2,3,4)) +
scale_x_log10(breaks=c(1,2,4,6,8,10,12)) +
# minimal theme & customizations
theme_bw() +
theme(legend.position=c(0.86,0.30),
#legend.justification = c("right","top"),
panel.grid.major = element_line(colour = "whitesmoke"),
panel.grid.minor=element_blank(),
legend.background = element_rect(colour = 'darkgrey', fill = 'white', linetype='solid'))
#ggsave(here("Produce_MAKEs_Spoken_chatterplot_min3.svg"), dpi = 300, width =23, height = 15, units = "cm")
#### Chatterplot: inspired by https://towardsdatascience.com/rip-wordclouds-long-live-chatterplots-e76a76896098
produce <- read.csv(file = here("Produce.csv"), sep = "\t", stringsAsFactors = TRUE)
summary(produce$USAS)
str(produce)
spoken_produce <- produce %>% filter(Subcorpus=="BNCspoken" | Subcorpus =="spoken")
spoken_produce$Subcorpus <- droplevels(spoken_produce$Subcorpus)
table(spoken_produce$Collocate_lemma, spoken_produce$Subcorpus)
comp <- round(prop.table(table(spoken_produce$Collocate_lemma, spoken_produce$Subcorpus), 2), 4)*100
comp <- as.data.frame(unclass(comp))
comp$Collocate_lemma <- row.names(comp)
head(comp)
comp <- inner_join(comp, spoken_produce[,6:7], by = "Collocate_lemma")
comp <- distinct(comp)
head(comp)
suffrager <- c(palettes_d$suffrager$classic[1], palettes_d$suffrager$CarolMan[3:4], palettes_d$suffrager$london, palettes_d$suffrager$oxon[c(1,5)])
levels(as.factor(comp$spoken))
## With minimal threshold of just min 2 occurrences
comp %>% filter(spoken > 0.57 | BNCspoken > 0.57) %>%
ggplot(aes(spoken, BNCspoken, label = Collocate_lemma)) +
# ggrepel geom, make arrows transparent, color by rank, size by n
geom_point(aes(colour=USAS)) +
geom_text_repel(min.segment.length = 0.3, segment.alpha = 0.2, force = 0.4,
aes(colour=USAS), show_guide = F) +
scale_colour_manual(values = suffrager,
name = "Semantic field") +
# set color gradient & customize legend
geom_abline(color = "gray40", lty = 2) +
# set word size range & turn off legend
labs(y = "% of MAKE 'produce' collocations in Spoken BNC2014 sample", x = "% of 'produce' MAKEs in Textbook Conversation") +
scale_y_log10(breaks=c(1,2,3,4)) +
scale_x_log10(breaks=c(1,2,4,6,8,10,12)) +
# minimal theme & customizations
theme_bw() +
theme(legend.position=c(0.86,0.35),
#legend.justification = c("right","top"),
panel.grid.major = element_line(colour = "whitesmoke"),
panel.grid.minor=element_blank(),
legend.background = element_rect(colour = 'darkgrey', fill = 'white', linetype='solid'))
#ggsave(here("Produce_MAKEs_Spoken_chatterplot.svg"), dpi = 300, width =22, height = 15, units = "cm")
## With higher threshold of min. 3 occurrences
comp %>% filter(spoken > 1.04 | BNCspoken > 1.11) %>%
ggplot(aes(spoken, BNCspoken, label = Collocate_lemma)) +
# ggrepel geom, make arrows transparent, color by rank, size by n
geom_point(aes(colour=USAS)) +
geom_text_repel(min.segment.length = 0.3, segment.alpha = 0.2, force = 0.4,
aes(colour=USAS), show_guide = F) +
scale_colour_manual(values = suffrager,
name = "Semantic field") +
# set color gradient & customize legend
geom_abline(color = "gray40", lty = 2) +
# set word size range & turn off legend
labs(y = "% of MAKE 'produce' collocations in Spoken BNC2014 sample", x = "% of 'produce' MAKEs in Textbook Conversation") +
scale_y_log10(breaks=c(1,2,3,4)) +
scale_x_log10(breaks=c(1,2,4,6,8,10,12)) +
# minimal theme & customizations
theme_bw() +
theme(legend.position=c(0.86,0.30),
#legend.justification = c("right","top"),
panel.grid.major = element_line(colour = "whitesmoke"),
panel.grid.minor=element_blank(),
legend.background = element_rect(colour = 'darkgrey', fill = 'white', linetype='solid'))
#ggsave(here("Produce_MAKEs_Spoken_chatterplot_min3.svg"), dpi = 300, width =23, height = 15, units = "cm")
#### Chatterplot: inspired by https://towardsdatascience.com/rip-wordclouds-long-live-chatterplots-e76a76896098
produce <- read.csv(file = here("Produce.csv"), sep = "\t", stringsAsFactors = TRUE)
summary(produce$USAS)
str(produce)
spoken_produce <- produce %>% filter(Subcorpus=="BNCspoken" | Subcorpus =="spoken")
spoken_produce$Subcorpus <- droplevels(spoken_produce$Subcorpus)
table_spoken_produce <- table(spoken_produce$Collocate_lemma, spoken_produce$Subcorpus)
head(table_spoken_produce)
comp <- round(prop.table(table(spoken_produce$Collocate_lemma, spoken_produce$Subcorpus), 2), 4)*100
comp <- as.data.frame(unclass(comp))
comp$Collocate_lemma <- row.names(comp)
head(comp)
comp <- inner_join(comp, spoken_produce[,6:7], by = "Collocate_lemma")
comp <- distinct(comp)
head(comp)
suffrager <- c(palettes_d$suffrager$classic[1], palettes_d$suffrager$CarolMan[3:4], palettes_d$suffrager$london, palettes_d$suffrager$oxon[c(1,5)])
levels(as.factor(comp$spoken))
## With minimal threshold of just min 2 occurrences
comp %>% filter(spoken > 0.57 | BNCspoken > 0.57) %>%
ggplot(aes(spoken, BNCspoken, label = Collocate_lemma)) +
# ggrepel geom, make arrows transparent, color by rank, size by n
geom_point(aes(colour=USAS)) +
geom_text_repel(min.segment.length = 0.3, segment.alpha = 0.2, force = 0.4,
aes(colour=USAS), show_guide = F) +
scale_colour_manual(values = suffrager,
name = "Semantic field") +
# set color gradient & customize legend
geom_abline(color = "gray40", lty = 2) +
# set word size range & turn off legend
labs(y = "% of MAKE 'produce' collocations in Spoken BNC2014 sample", x = "% of 'produce' MAKEs in Textbook Conversation") +
scale_y_log10(breaks=c(1,2,3,4)) +
scale_x_log10(breaks=c(1,2,4,6,8,10,12)) +
# minimal theme & customizations
theme_bw() +
theme(legend.position=c(0.86,0.35),
#legend.justification = c("right","top"),
panel.grid.major = element_line(colour = "whitesmoke"),
panel.grid.minor=element_blank(),
legend.background = element_rect(colour = 'darkgrey', fill = 'white', linetype='solid'))
#ggsave(here("Produce_MAKEs_Spoken_chatterplot.svg"), dpi = 300, width =22, height = 15, units = "cm")
## With higher threshold of min. 3 occurrences
comp %>% filter(spoken > 1.04 | BNCspoken > 1.11) %>%
ggplot(aes(spoken, BNCspoken, label = Collocate_lemma)) +
# ggrepel geom, make arrows transparent, color by rank, size by n
geom_point(aes(colour=USAS)) +
geom_text_repel(min.segment.length = 0.3, segment.alpha = 0.2, force = 0.4,
aes(colour=USAS), show_guide = F) +
scale_colour_manual(values = suffrager,
name = "Semantic field") +
# set color gradient & customize legend
geom_abline(color = "gray40", lty = 2) +
# set word size range & turn off legend
labs(y = "% of MAKE 'produce' collocations in Spoken BNC2014 sample", x = "% of 'produce' MAKEs in Textbook Conversation") +
scale_y_log10(breaks=c(1,2,3,4)) +
scale_x_log10(breaks=c(1,2,4,6,8,10,12)) +
# minimal theme & customizations
theme_bw() +
theme(legend.position=c(0.86,0.30),
#legend.justification = c("right","top"),
panel.grid.major = element_line(colour = "whitesmoke"),
panel.grid.minor=element_blank(),
legend.background = element_rect(colour = 'darkgrey', fill = 'white', linetype='solid'))
#ggsave(here("Produce_MAKEs_Spoken_chatterplot_min3.svg"), dpi = 300, width =23, height = 15, units = "cm")
### Wordclouds for collocates ###
produce <- read.csv(file = here("Produce.csv"), sep = "\t", stringsAsFactors = TRUE)
summary(produce$USAS)
produce_freq <- produce %>%
group_by(Collocate_lemma, USAS) %>%
summarize(Frequency=n())
# Now we can apply the colour scheme:
colours1 <- RColorBrewer::brewer.pal(n=8, name="Set1")
colours2 <- RColorBrewer::brewer.pal(n=10, name="RdGy")
colours <- c(colours1, colours2[c(1,9,10)]) # We need a total of 12 colours for 12 categories
### Now a wordcloud for each subcorpus ###
produceBNC <- produce[produce$Subcorpus=="BNCspoken",]
produceTxBspoken <- produce[produce$Subcorpus=="spoken",]
produce_freq_BNC <- produceBNC %>%
group_by(Collocate_lemma, USAS) %>%
summarize(Frequency=n())
produce_freq_TxBspoken <- produceTxBspoken %>%
group_by(Collocate_lemma, USAS) %>%
summarize(Frequency=n())
anchor = 10
#png(filename = here("produce_BNC_categories.png"), width = 1300, height = 1100, res = 300)
#svg(filename=here("produce_BNC_collocates.svg"),
#width=9,
#height=6,
#pointsize=11)
par(mar = rep(0, 4))
produce_freq_BNC$colors <- colours[match(produce_freq_BNC$USAS, levels(produce_freq_BNC$USAS))]
wordcloud(produce_freq_BNC$Collocate_lemma,
produce_freq_BNC$Frequency,
min.freq=2,
rot.per=0,
colors=produce_freq_BNC$colors,
ordered.colors=TRUE,
random.order=FALSE,
scale = c(10*max(table(produce_freq_BNC$Collocate_lemma))/anchor, 0.5))
dev.off()
#png(filename = here("produce_TxBspoken_categories.png"), width = 1300, height = 1100, res = 300)
#svg(filename=here("produce_TxBSpoken_collocates.svg"),
#width=9,
#height=6,
#pointsize=11)
par(mar = rep(0, 4))
produce_freq_TxBspoken$colors <- colours[match(produce_freq_TxBspoken$USAS, levels(produce_freq_TxBspoken$USAS))]
wordcloud(produce_freq_TxBspoken$Collocate_lemma,
produce_freq_TxBspoken$Frequency,
min.freq=2,
rot.per=0,
colors=produce_freq_TxBspoken$colors,
ordered.colors=TRUE,
random.order=FALSE,
scale = c(40*max(table(produce_freq_BNC$Collocate_lemma))/anchor, 0.5))
dev.off()
produce_sem <- read.csv(file = here("Produce_semantics.csv"), sep = "\t", stringsAsFactors = TRUE) # This data set was produced in the MAKE_produce_wordcloud_lollipop.R script
### Now onto a comparison of semantic categories between TxB narrative and Youth Fiction ###
###
# Reorder data by size of difference in frequency #
nar_sem = produce_sem %>%
rowwise() %>% mutate(dif = sqrt((Youth_FictionPer - TxBnarPer)^2)) %>%
arrange(desc(dif)) %>%
mutate(Semantics=factor(Semantics, Semantics))
nar_sem$Semantics <- factor(nar_sem$Semantics, levels = nar_sem$Semantics[order(nar_sem$dif)])
### Chi-squared tests ##
cols = c("Youth_Fiction", "TxBnar")
semantics_test = chisq.test(nar_sem[,cols])
# Comparing the two distributions #
print(semantics_test)
# X-squared = 22.119, df = 10, p-value = 0.01451
# Comparing proportions for each semantic category #
results = data.frame(Semantics=nar_sem$Semantics, p=1)
for(i in 1:nrow(results)){
category = nar_sem$Semantics[i]
row = nar_sem[nar_sem$Semantics==category,cols]
other_rows = nar_sem[nar_sem$Semantics!=category,cols]
collapsed = rbind(row, colSums(other_rows))
category_test = fisher.test(collapsed)
results$p[i] = category_test$p.value
results$OddsRatio[i] = category_test$estimate
results$conf.int.lower[i] = category_test$conf.int[1]
results$conf.int.upper[i] = category_test$conf.int[2]
}
results$p = p.adjust(results$p, method='none')
## We can add the stars for low p-values using ifelse(). ##
#criterion = 0.05
#results$Star = ifelse(results$p < criterion, '**', '')
## But this package makes it much quicker! ##
results$Star <- stars.pval(results$p)
results = results %>%
mutate(Star = replace(Star, Star==".", "˘")) # Change dot to a higher symbol such as º or ˘
# Check the outcome.
print(results)
## Lollipop chart ##
#svg(filename=here("MAKE_produce_Fiction.svg"), width=14, height=7, pointsize=12)
fig = ggplot(nar_sem) +
geom_segment(aes(x=Semantics, xend=Semantics, y=Youth_FictionPer, yend=TxBnarPer), color="black") +
geom_point(aes(x=Semantics, y=Youth_FictionPer), color=rgb(153,0,0, max = 255, alpha = 255), size=5) +
geom_point(aes(x=Semantics, y=TxBnarPer), color=rgb(252, 141, 89, max = 255, alpha = 220), size=5) +
coord_flip() +
theme_minimal() +
theme(
panel.border = element_blank(),
axis.text=element_text(size=16, colour = "black"),
axis.title.x=element_text(size=16, face = "bold")
) +
scale_x_discrete(limits=c(levels(nar_sem$Semantics),"")) +
xlab("") +
ylab("% of MAKE collocations in the (Ai) produce sense") + # Adds an empty factor to give space for legend as recommended in: https://stackoverflow.com/questions/16788180/r-ggplot-extend-the-range-of-a-category-x-axis
theme(plot.margin=margin(10,10,10,10))
# +  labs(title = "Differences in semantic fields of collocates of MAKE in the produce sense (Ai).")
fig1 = fig +
geom_text(data=data.frame(), aes(x="Buildings & House", y= 5, label="Textbook Fiction"),color=rgb(252, 141, 89, max = 255, alpha = 255), hjust=1, size=6, fontface = "bold", nudge_x = 0.7) +
geom_text(data=data.frame(), aes(x="Buildings & House", y= 14, label="Youth Fiction Sample"), color=rgb(153,0,0, max = 255, alpha = 255), hjust=0, size=6, fontface = "bold", nudge_x = 0.7)
## This is to help us place the stars in the middle of the bars ##
cols = c("Youth_FictionPer", "TxBnarPer")
results$y = rowMeans(nar_sem[,cols])
fig2 = fig1 +
geom_text(aes(x=Semantics, y=y, label=Star), data=results, size=9)
print(fig2)
dev.off()
#ggsave(here("produce_USAS_Fiction.png"), width = 24, height = 12, units = "cm", dpi = 300)
produce <- read.csv(file = here("Produce.csv"), sep = "\t", stringsAsFactors = TRUE)
fiction_produce <- produce %>% filter(Subcorpus=="Youth_Fiction" | Subcorpus =="TxBnar")
fiction_produce$Subcorpus <- droplevels(fiction_produce$Subcorpus)
table(fiction_produce$Collocate_lemma, fiction_produce$Subcorpus)
comp <- round(prop.table(table(fiction_produce$Collocate_lemma, fiction_produce$Subcorpus), 2), 4)*100
comp <- as.data.frame(unclass(comp))
comp$Collocate_lemma <- row.names(comp)
head(comp)
comp <- inner_join(comp, fiction_produce[,6:7], by = "Collocate_lemma")
comp <- distinct(comp)
head(comp)
suffrager <- c(palettes_d$suffrager$CarolMan[2:4], palettes_d$suffrager$classic[1:2], palettes_d$suffrager$london, palettes_d$suffrager$oxon[c(1,5)])
levels(as.factor(comp$Youth_Fiction))
## With minimal threshold of just min 2 occurrences
comp %>% filter(TxBnar > 1.09 | Youth_Fiction > 1.17) %>%
ggplot(aes(TxBnar, Youth_Fiction, label = Collocate_lemma)) +
# ggrepel geom, make arrows transparent, color by rank, size by n
geom_text_repel(segment.alpha = 0,
aes(colour=USAS)) +
scale_colour_manual(values = suffrager,
name = "Semantic field") +
# set color gradient & customize legend
geom_abline(color = "gray40", lty = 2) +
# set word size range & turn off legend
labs(y = "% of MAKE 'produce' collocations in Spoken BNC2014 sample", x = "% of MAKE 'produce' collocations in Textbook Conversation") +
scale_y_log10(breaks=c(1,1.5,2,2.5,3)) +
scale_x_log10(breaks=c(1,2,4,6,8)) +
# minimal theme & customizations
theme_bw() +
theme(legend.position=c(0.86,0.35),
#legend.justification = c("right","top"),
panel.grid.major = element_line(colour = "whitesmoke"),
panel.grid.minor=element_blank(),
legend.background = element_rect(colour = 'darkgrey', fill = 'white', linetype='solid'))
delexical <- read.csv(here("Delexical_MAKE2.csv"), sep = ",", stringsAsFactors = TRUE)
str(delexical)
table(delexical$Collocate_lemma, delexical$Register)
anchor <- 20 # Should be maximum frequency in any of the wordclouds to be compared
# Youth Fiction #
delexicalYF <- delexical[delexical$Register == "Youth_Fiction",]
#png(filename = here("Delexical_YF_minFreq2.png"), width = 1200, height = 1000, res = 300) # Too low a quality?
#svg(filename=here("Delexical_YF_minFreq2.svg"),
#width=4,
#height=3,
#pointsize=11)
wordcloud::wordcloud(delexicalYF$Collocate_lemma, min.freq = 2, rot.per = 0, colors=cols[9], scale = c(8*max(table(delexicalYF$Collocate_lemma))/anchor, 0.5))
dev.off()
# Textbook narrative #
delexicalTxBnar <- delexical[delexical$Register == "narrative",]
#png(filename = here("Delexical_TxBnar_minFreq2.png"), width = 1300, height = 1100, res = 300)
#svg(filename=here("Delexical_TxBnar_minFreq2.svg"),
#width=5,
#height=3,
#pointsize=11)
wordcloud(delexicalTxBnar$Collocate_lemma, min.freq = 2, rot.per = 0, colors=cols[6], scale = c(6*max(table(delexicalTxBnar$Collocate_lemma))/anchor, 0.5))
dev.off()
# BNC spoken #
delexicalBNC <- delexical[delexical$Register == "BNC_Spoken",]
#png(filename = here("Delexical_BNCs_minFreq2.png"), width = 2000, height = 1800, res = 300) # Too low a quality?
#svg(filename=here("Delexical_BNCs_minFreq2.svg"),
# width=6,
#height=6,
#pointsize=11)
wordcloud(delexicalBNC$Collocate_lemma, min.freq = 2, rot.per = 0, colors=cols[9], scale = c(8*max(table(delexicalBNC$Collocate_lemma))/anchor, 0.5))
dev.off() # To save plot
# Textbook spoken #
delexicalTxBspoken <- delexical[delexical$Register == "TxB_spoken",]
#png(filename = here("Delexical_TxBspoken_minFreq2.png"), width = 2000, height = 1800, res = 300)
#svg(filename=here("Delexical_TxBspoken_minFreq2.svg"),
#width=5,
#height=10,
#pointsize=11)
wordcloud(delexicalTxBspoken$Collocate_lemma, min.freq = 2, rot.per = 0, colors=cols[6], scale = c(4*max(table(delexicalTxBspoken$Collocate_lemma))/anchor, 0.5))
delexical <- read.csv(here("Delexical_MAKE2.csv"), sep = ",", stringsAsFactors = TRUE)
delexical <- read.csv(here("Delexical_MAKE2.csv"), sep = ",", stringsAsFactors = TRUE)
PV <- read.csv(here("MAKE_PhrasalVerbs_Conc.csv"))
t(table(PV$Corpus, PV$PV))
summary(as.factor(PV$Corpus))
table(PV$Corpus, PV$Level)
TxBSpokenCaus <- read.csv(here("Textbook_MAKE_Causative_Constructions.csv"), stringsAsFactors = TRUE)
str(TxBSpokenCaus)
TxBSpokenCaus$Register = "Textbook Conversation"
BNC2014Caus <- read.csv(here("SpokenBNC2014_MAKE_Causative_Constructions.csv"), stringsAsFactors = TRUE)
str(BNC2014Caus)
BNC2014Caus <- BNC2014Caus %>% select(-c(Filename))
BNC2014Caus$Register = "Spoken BNC2014"
BNC2014Caus$Level = "Spoken BNC2014"
BNC2014Caus$Series = "Spoken BNC2014"
caus <- rbind(TxBSpokenCaus, BNC2014Caus) %>% select(-Semantic_prosody)
caus$Register <- as.factor(caus$Register)
str(caus)
table(caus$Cause_Cx, caus$Register)
round(prop.table(table(caus$Cause_Cx, caus$Register), 2), 4)*100
vcd::assoc(caus$Cause_Cx ~ caus$Register, shade = TRUE, varnames = FALSE)
#Chatterplot: inspired by https://towardsdatascience.com/rip-wordclouds-long-live-chatterplots-e76a76896098
comp <- round(prop.table(table(caus$Collocate, caus$Register), 2), 4)*100
comp <- as.data.frame(unclass(comp))
comp$Collocate <- row.names(comp)
comp <- inner_join(comp, caus[,7:8], by = "Collocate")
comp <- distinct(comp)
str(comp)
comp %>%
filter(`Textbook Conversation` > 0.95 | `Spoken BNC2014` > 0.95) %>%
ggplot(aes(`Textbook Conversation`, `Spoken BNC2014`, colour = Cause_Cx, label = Collocate)) +
# ggrepel geom, make arrows transparent, color by rank, size by n
geom_point(aes(colour=Cause_Cx)) +
geom_text_repel(min.segment.length = 0.5, segment.alpha = 0.4, force = 1, max.overlaps = 20, aes(colour=Cause_Cx), show.legend = F) +
scale_colour_manual(breaks = c("adjectiveCx", "nounCx", "verbalCxAA"),
values = palettes_d$suffrager$oxon[c(2,1,3)],
labels = c("[X MAKE Y AdjP]", "[X MAKE (Y) NP]", "[X MAKE Y Vinf]"),
name = "Construction type") +
# set color gradient & customize legend
geom_abline(color = "gray40", lty = 2) +
# set word size range & turn off legend
labs(y = "% of causative MAKEs in the Spoken BNC2014 sample", x = "% of causative MAKEs in Textbook Conversation") +
scale_y_log10(breaks = c(1,2,4,6,8,10)) +
scale_x_log10(breaks = c(1,2,4,6,8,10)) +
# minimal theme & customizations
theme_bw() +
theme(legend.position=c(0.97,0.40),
legend.justification = c("right","top"),
panel.grid.major = element_line(colour = "whitesmoke"),
panel.grid.minor=element_blank(),
legend.background = element_rect(colour = 'darkgrey', fill = 'white', linetype='solid'))
#ggsave(here("Causative_Spoken_chatterplot.svg"), dpi=300, width=20, height=20, units="cm")
dev.off()
comp %>%
filter(`Textbook Conversation` > 0.95 | `Spoken BNC2014` > 0.95) %>%
ggplot(aes(`Textbook Conversation`, `Spoken BNC2014`, colour = Cause_Cx, label = Collocate)) +
# ggrepel geom, make arrows transparent, color by rank, size by n
geom_point(aes(colour=Cause_Cx)) +
geom_text_repel(min.segment.length = 0.5, segment.alpha = 0.4, force = 1, max.overlaps = 20, aes(colour=Cause_Cx), show.legend = F) +
scale_colour_manual(breaks = c("adjectiveCx", "nounCx", "verbalCxAA"),
values = palettes_d$suffrager$oxon[c(2,1,3)],
labels = c("[X MAKE Y AdjP]", "[X MAKE (Y) NP]", "[X MAKE Y Vinf]"),
name = "Construction type") +
# set color gradient & customize legend
geom_abline(color = "gray40", lty = 2) +
# set word size range & turn off legend
labs(y = "% of causative MAKEs in the Spoken BNC2014 sample", x = "% of causative MAKEs in Textbook Conversation") +
scale_y_log10(breaks = c(1,2,4,6,8,10)) +
scale_x_log10(breaks = c(1,2,4,6,8,10)) +
# minimal theme & customizations
theme_bw() +
theme(legend.position=c(0.97,0.40),
legend.justification = c("right","top"),
panel.grid.major = element_line(colour = "whitesmoke"),
panel.grid.minor=element_blank(),
legend.background = element_rect(colour = 'darkgrey', fill = 'white', linetype='solid'))
sessionInfo()
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, message=FALSE, paged.print=TRUE, fig.width = 10, warning=FALSE)
#renv::restore() # Restore the project's dependencies from the lockfile to ensure that same package versions are used as in the original thesis.
library(here)
library(ggsignif)
library(ggrepel)
library(paletteer)
require(gtools)
library(RColorBrewer)
library(tidyverse)
library(wordcloud)
renv::init()
